{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:22.294721Z",
     "iopub.status.busy": "2020-09-26T11:52:22.293741Z",
     "iopub.status.idle": "2020-09-26T11:52:22.296835Z",
     "shell.execute_reply": "2020-09-26T11:52:22.296084Z"
    },
    "papermill": {
     "duration": 0.037317,
     "end_time": "2020-09-26T11:52:22.297000",
     "exception": false,
     "start_time": "2020-09-26T11:52:22.259683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:22.358024Z",
     "iopub.status.busy": "2020-09-26T11:52:22.357168Z",
     "iopub.status.idle": "2020-09-26T11:52:22.360520Z",
     "shell.execute_reply": "2020-09-26T11:52:22.359638Z"
    },
    "papermill": {
     "duration": 0.037125,
     "end_time": "2020-09-26T11:52:22.360667",
     "exception": false,
     "start_time": "2020-09-26T11:52:22.323542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm \n",
    "# for some fancy loading bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:22.421179Z",
     "iopub.status.busy": "2020-09-26T11:52:22.420025Z",
     "iopub.status.idle": "2020-09-26T11:52:22.423778Z",
     "shell.execute_reply": "2020-09-26T11:52:22.423103Z"
    },
    "papermill": {
     "duration": 0.036022,
     "end_time": "2020-09-26T11:52:22.423934",
     "exception": false,
     "start_time": "2020-09-26T11:52:22.387912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir = \"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028029,
     "end_time": "2020-09-26T11:52:22.479362",
     "exception": false,
     "start_time": "2020-09-26T11:52:22.451333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reading the json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02711,
     "end_time": "2020-09-26T11:52:22.537098",
     "exception": false,
     "start_time": "2020-09-26T11:52:22.509988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Json files are stored in the form of collection of dictionary objects as strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:22.597626Z",
     "iopub.status.busy": "2020-09-26T11:52:22.596826Z",
     "iopub.status.idle": "2020-09-26T11:52:22.605576Z",
     "shell.execute_reply": "2020-09-26T11:52:22.606343Z"
    },
    "papermill": {
     "duration": 0.04198,
     "end_time": "2020-09-26T11:52:22.606532",
     "exception": false,
     "start_time": "2020-09-26T11:52:22.564552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"article_link\": \"https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5\", \"headline\": \"former versace store clerk sues over secret 'black code' for minority shoppers\", \"is_sarcastic\": 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(dir) as f:\n",
    "    for i in f:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027288,
     "end_time": "2020-09-26T11:52:22.663007",
     "exception": false,
     "start_time": "2020-09-26T11:52:22.635719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "By using the json library, we can convert this string into a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:22.725280Z",
     "iopub.status.busy": "2020-09-26T11:52:22.724362Z",
     "iopub.status.idle": "2020-09-26T11:52:22.730191Z",
     "shell.execute_reply": "2020-09-26T11:52:22.729457Z"
    },
    "papermill": {
     "duration": 0.040192,
     "end_time": "2020-09-26T11:52:22.730326",
     "exception": false,
     "start_time": "2020-09-26T11:52:22.690134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_link  |  https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5\n",
      "headline  |  former versace store clerk sues over secret 'black code' for minority shoppers\n",
      "is_sarcastic  |  0\n"
     ]
    }
   ],
   "source": [
    "with open(dir) as f:\n",
    "    \n",
    "    for i in f:\n",
    "        json_loaded = json.loads(i)\n",
    "        \n",
    "        for key in json_loaded:\n",
    "            print(key,\" | \",  json_loaded[key])\n",
    "            \n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027415,
     "end_time": "2020-09-26T11:52:22.785745",
     "exception": false,
     "start_time": "2020-09-26T11:52:22.758330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we can store the relevant information like headline and is_sarcastic as seperate entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:22.850641Z",
     "iopub.status.busy": "2020-09-26T11:52:22.849623Z",
     "iopub.status.idle": "2020-09-26T11:52:23.052656Z",
     "shell.execute_reply": "2020-09-26T11:52:23.052029Z"
    },
    "papermill": {
     "duration": 0.239211,
     "end_time": "2020-09-26T11:52:23.052878",
     "exception": false,
     "start_time": "2020-09-26T11:52:22.813667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences/headlines: 26709\n",
      "A random entry and its label:\n",
      "\n",
      "rescuers heroically help beached garbage back into ocean\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "headlines = []\n",
    "labels = []\n",
    "\n",
    "with open(dir) as f:\n",
    "    for i in f:\n",
    "        json_loaded = json.loads(i)\n",
    "        headlines.append(json_loaded['headline'])\n",
    "        labels.append(json_loaded['is_sarcastic'])\n",
    "\n",
    "print(f\"Number of sentences/headlines: {len(headlines)}\")\n",
    "print(\"A random entry and its label:\\n\")\n",
    "print(headlines[42])\n",
    "print(labels[42])\n",
    "# is_sarcastic = 1 for a sarcastic comment and 0 for a non sarcastic one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028266,
     "end_time": "2020-09-26T11:52:23.112062",
     "exception": false,
     "start_time": "2020-09-26T11:52:23.083796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clean the acquired data\n",
    "Now its time to clean the sentences. \n",
    "This usually consists of 4 parts:\n",
    "1. Remove unnecessary details like links and punctuations \n",
    "2. Convert each word to its base form\n",
    "3. Convert the words to numerical tokens \n",
    "4. Pad these tokenized sequences to a fixed length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:23.175814Z",
     "iopub.status.busy": "2020-09-26T11:52:23.175023Z",
     "iopub.status.idle": "2020-09-26T11:52:25.094224Z",
     "shell.execute_reply": "2020-09-26T11:52:25.095340Z"
    },
    "papermill": {
     "duration": 1.954998,
     "end_time": "2020-09-26T11:52:25.095577",
     "exception": false,
     "start_time": "2020-09-26T11:52:23.140579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035071,
     "end_time": "2020-09-26T11:52:25.170847",
     "exception": false,
     "start_time": "2020-09-26T11:52:25.135776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Removing the links from the text\n",
    "By using some Regex we can filter out the website link from the text with just one line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:25.266617Z",
     "iopub.status.busy": "2020-09-26T11:52:25.265676Z",
     "iopub.status.idle": "2020-09-26T11:52:25.269849Z",
     "shell.execute_reply": "2020-09-26T11:52:25.270463Z"
    },
    "papermill": {
     "duration": 0.054874,
     "end_time": "2020-09-26T11:52:25.270624",
     "exception": false,
     "start_time": "2020-09-26T11:52:25.215750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My github page is  please follow me '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "s = \"My github page is https://github.com/The-Bread please follow me \"\n",
    "text = re.sub(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', '', s, flags=re.MULTILINE)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031673,
     "end_time": "2020-09-26T11:52:25.334578",
     "exception": false,
     "start_time": "2020-09-26T11:52:25.302905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Replacing words with their base forms\n",
    "SnowballStemmer and tokenize from nltk helps to reduce similar words from different tenses to the same form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:25.401748Z",
     "iopub.status.busy": "2020-09-26T11:52:25.400968Z",
     "iopub.status.idle": "2020-09-26T11:52:25.422028Z",
     "shell.execute_reply": "2020-09-26T11:52:25.421289Z"
    },
    "papermill": {
     "duration": 0.057189,
     "end_time": "2020-09-26T11:52:25.422152",
     "exception": false,
     "start_time": "2020-09-26T11:52:25.364963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy buy . like . seem . doe n't , doe , easi ,\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "s = \"Buy buys . Likes. seems . Doesn't, does, easy,\"\n",
    "\n",
    "s = s.lower()\n",
    "token = nltk.word_tokenize(s)\n",
    "sentence = [nltk.stem.SnowballStemmer('english').stem(word) for word in token]\n",
    "\n",
    "print(\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029613,
     "end_time": "2020-09-26T11:52:25.481776",
     "exception": false,
     "start_time": "2020-09-26T11:52:25.452163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Combine both the steps to a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:25.549928Z",
     "iopub.status.busy": "2020-09-26T11:52:25.549076Z",
     "iopub.status.idle": "2020-09-26T11:52:25.552355Z",
     "shell.execute_reply": "2020-09-26T11:52:25.552923Z"
    },
    "papermill": {
     "duration": 0.04161,
     "end_time": "2020-09-26T11:52:25.553098",
     "exception": false,
     "start_time": "2020-09-26T11:52:25.511488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = re.sub(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', '', sentence, flags=re.MULTILINE)\n",
    "    sentence = sentence.lower()\n",
    "    token = nltk.word_tokenize(sentence)\n",
    "    sentence = [nltk.stem.SnowballStemmer('english').stem(word) for word in token]\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:25.621272Z",
     "iopub.status.busy": "2020-09-26T11:52:25.620230Z",
     "iopub.status.idle": "2020-09-26T11:52:25.624581Z",
     "shell.execute_reply": "2020-09-26T11:52:25.623969Z"
    },
    "papermill": {
     "duration": 0.0416,
     "end_time": "2020-09-26T11:52:25.624705",
     "exception": false,
     "start_time": "2020-09-26T11:52:25.583105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "longtime teacher retires without changing a single student's life\n",
      "\n",
      "Preprocessed: \n",
      "longtim teacher retir without chang a singl student 's life\n"
     ]
    }
   ],
   "source": [
    "example = headlines[52]\n",
    "print(\"Original:\")\n",
    "print(example)\n",
    "print(\"\\nPreprocessed: \")\n",
    "print(preprocess(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030533,
     "end_time": "2020-09-26T11:52:25.686563",
     "exception": false,
     "start_time": "2020-09-26T11:52:25.656030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Tokenize the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03075,
     "end_time": "2020-09-26T11:52:25.748287",
     "exception": false,
     "start_time": "2020-09-26T11:52:25.717537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next step is to convert the sentences into a sequence of numbers corresponding to each word.\n",
    "The numbers are given according to the frequency of that particular word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:25.816706Z",
     "iopub.status.busy": "2020-09-26T11:52:25.815898Z",
     "iopub.status.idle": "2020-09-26T11:52:32.171230Z",
     "shell.execute_reply": "2020-09-26T11:52:32.170399Z"
    },
    "papermill": {
     "duration": 6.39215,
     "end_time": "2020-09-26T11:52:32.171377",
     "exception": false,
     "start_time": "2020-09-26T11:52:25.779227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:32.243186Z",
     "iopub.status.busy": "2020-09-26T11:52:32.242186Z",
     "iopub.status.idle": "2020-09-26T11:52:32.246588Z",
     "shell.execute_reply": "2020-09-26T11:52:32.245821Z"
    },
    "papermill": {
     "duration": 0.04325,
     "end_time": "2020-09-26T11:52:32.246710",
     "exception": false,
     "start_time": "2020-09-26T11:52:32.203460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Sentence: Where are you\n",
      "\n",
      "\n",
      "Tokenized Sequence: [6, 1, 2]\n",
      "\n",
      "\n",
      "{'are': 1, 'you': 2, 'hello': 3, 'how': 4, 'today': 5, 'where': 6}\n"
     ]
    }
   ],
   "source": [
    "S = [\"Hello\", \"How are you today\", \"Where are you\"]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(S)\n",
    "print(f\"Actual Sentence: {S[2]}\\n\\n\")\n",
    "\n",
    "S = tokenizer.texts_to_sequences(S)\n",
    "print(f\"Tokenized Sequence: {S[2]}\\n\\n\")\n",
    "\n",
    "print(tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031099,
     "end_time": "2020-09-26T11:52:32.309144",
     "exception": false,
     "start_time": "2020-09-26T11:52:32.278045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Pad these sequences. \n",
    "###### We can see that sentences can have varying length which might be problematic when passing through a neural network which requires a fixed input size.\n",
    "###### Hence we fill the shorter sentences with 0 (since that is not reserved for any other word in the tokenizer index) and we truncate the longer sentences. We can either do these from the left side(pre) or the right side(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:32.380289Z",
     "iopub.status.busy": "2020-09-26T11:52:32.379103Z",
     "iopub.status.idle": "2020-09-26T11:52:32.383581Z",
     "shell.execute_reply": "2020-09-26T11:52:32.383020Z"
    },
    "papermill": {
     "duration": 0.043292,
     "end_time": "2020-09-26T11:52:32.383712",
     "exception": false,
     "start_time": "2020-09-26T11:52:32.340420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences:\n",
      "[[3 0 0]\n",
      " [4 1 2]\n",
      " [6 1 2]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(S, maxlen=3, padding='post', truncating='post')\n",
    "print(f\"Padded sequences:\\n{padded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:32.455780Z",
     "iopub.status.busy": "2020-09-26T11:52:32.454740Z",
     "iopub.status.idle": "2020-09-26T11:52:32.457585Z",
     "shell.execute_reply": "2020-09-26T11:52:32.458135Z"
    },
    "papermill": {
     "duration": 0.042633,
     "end_time": "2020-09-26T11:52:32.458303",
     "exception": false,
     "start_time": "2020-09-26T11:52:32.415670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_data(X):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(X, maxlen=15, padding='post', truncating='post')\n",
    "    X = np.array(X) # convert to numpy array \n",
    "    return X, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031958,
     "end_time": "2020-09-26T11:52:32.526474",
     "exception": false,
     "start_time": "2020-09-26T11:52:32.494516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Now to apply these methods to the headlines we got from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:32.602177Z",
     "iopub.status.busy": "2020-09-26T11:52:32.601115Z",
     "iopub.status.idle": "2020-09-26T11:52:47.276296Z",
     "shell.execute_reply": "2020-09-26T11:52:47.277112Z"
    },
    "papermill": {
     "duration": 14.717266,
     "end_time": "2020-09-26T11:52:47.277332",
     "exception": false,
     "start_time": "2020-09-26T11:52:32.560066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stemming the Headlines: 100%|██████████| 26709/26709 [00:13<00:00, 2012.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming complete\n",
      "\n",
      "Tokenizing the headlines\n",
      "Headlines Tokenized\n",
      "\n",
      "Input Shape: (26709, 15)\n",
      "Ouput Shape: (26709,)\n",
      "Final Output Shape: (26709, 1)\n"
     ]
    }
   ],
   "source": [
    "stemmed = [preprocess(s) for s in tqdm.tqdm(headlines, desc='Stemming the Headlines')]\n",
    "print(\"Stemming complete\")\n",
    "\n",
    "print(\"\\nTokenizing the headlines\")\n",
    "X, tokenizer = tokenize_data(stemmed)\n",
    "print(\"Headlines Tokenized\")\n",
    "print(f\"\\nInput Shape: {X.shape}\")\n",
    "\n",
    "Y = np.array(labels)\n",
    "print(f\"Ouput Shape: {Y.shape}\")\n",
    "\n",
    "# Reshape Y to make it 2D like the inputs\n",
    "Y = np.reshape(Y, (-1, 1))\n",
    "print(f\"Final Output Shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:47.465165Z",
     "iopub.status.busy": "2020-09-26T11:52:47.464326Z",
     "iopub.status.idle": "2020-09-26T11:52:47.468727Z",
     "shell.execute_reply": "2020-09-26T11:52:47.467978Z"
    },
    "papermill": {
     "duration": 0.098049,
     "end_time": "2020-09-26T11:52:47.468895",
     "exception": false,
     "start_time": "2020-09-26T11:52:47.370846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words tokenized:  17851\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words tokenized: \", len(tokenizer.word_index))\n",
    "num_words = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.086116,
     "end_time": "2020-09-26T11:52:47.641458",
     "exception": false,
     "start_time": "2020-09-26T11:52:47.555342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Making the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.086226,
     "end_time": "2020-09-26T11:52:47.814707",
     "exception": false,
     "start_time": "2020-09-26T11:52:47.728481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our goal is to make a model which will input a sentence, and output whether its sarcastic or not.\n",
    "##### For working on text based models , a special type of networks are used called RNN(Recurrent Neural Networks). \n",
    "More info about RNNs can be found [here](https://towardsdatascience.com/understanding-rnn-and-lstm-f7cdf6dfc14e). Alternatively, you can enroll in [this course](https://www.coursera.org/learn/natural-language-processing-tensorflow/) by DeepLearning.ai on coursera (Recommended).\n",
    "##### We will be using LSTM version of RNNs with Conv1D layers to make a simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:47.994595Z",
     "iopub.status.busy": "2020-09-26T11:52:47.993466Z",
     "iopub.status.idle": "2020-09-26T11:52:47.996954Z",
     "shell.execute_reply": "2020-09-26T11:52:47.997536Z"
    },
    "papermill": {
     "duration": 0.096525,
     "end_time": "2020-09-26T11:52:47.997717",
     "exception": false,
     "start_time": "2020-09-26T11:52:47.901192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, GlobalAveragePooling1D, Conv1D, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.099497,
     "end_time": "2020-09-26T11:52:48.189112",
     "exception": false,
     "start_time": "2020-09-26T11:52:48.089615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The Embedding layer:\n",
    "The Embedding layer is responsible for making sense out of the numbers assigned to the words. This layer maps each token as an N-dimensional vector. Similar vectors point near the same location on some dimension and might point away from each other on higher dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.087207,
     "end_time": "2020-09-26T11:52:48.365827",
     "exception": false,
     "start_time": "2020-09-26T11:52:48.278620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For example : words like cat, dog and pug might be located closely but on higher dimensions, cat might be farther from dog than the word pug (since it is a type of dog)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.088615,
     "end_time": "2020-09-26T11:52:48.547013",
     "exception": false,
     "start_time": "2020-09-26T11:52:48.458398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you want to build a highly accurate model, you would use the pretrained vectors for these words. Some examples are GloVe which offers vectors for a lot of commonly used words. They have also provided us with vectors with different Embedding Dimensions (50d, 100d, 200d, etc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:48.735905Z",
     "iopub.status.busy": "2020-09-26T11:52:48.735008Z",
     "iopub.status.idle": "2020-09-26T11:52:49.509734Z",
     "shell.execute_reply": "2020-09-26T11:52:49.508454Z"
    },
    "papermill": {
     "duration": 0.873054,
     "end_time": "2020-09-26T11:52:49.509980",
     "exception": false,
     "start_time": "2020-09-26T11:52:48.636926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 10)            178520    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 15, 15)            1560      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 6, 15)             2265      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 182,890\n",
      "Trainable params: 182,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(num_words+1, output_dim=10, input_length=15),\n",
    "    LSTM(15, dropout=0.1, return_sequences=True),\n",
    "    Conv1D(15, 10, activation='relu'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    \n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-26T11:52:49.696567Z",
     "iopub.status.busy": "2020-09-26T11:52:49.695435Z",
     "iopub.status.idle": "2020-09-26T11:53:15.034686Z",
     "shell.execute_reply": "2020-09-26T11:53:15.033933Z"
    },
    "papermill": {
     "duration": 25.433552,
     "end_time": "2020-09-26T11:53:15.034810",
     "exception": false,
     "start_time": "2020-09-26T11:52:49.601258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "835/835 [==============================] - 8s 9ms/step - loss: 0.4248 - acc: 0.7979\n",
      "Epoch 2/3\n",
      "835/835 [==============================] - 7s 9ms/step - loss: 0.3164 - acc: 0.8647\n",
      "Epoch 3/3\n",
      "835/835 [==============================] - 8s 9ms/step - loss: 0.2899 - acc: 0.8789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f776d47bed0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.259254,
     "end_time": "2020-09-26T11:53:15.556458",
     "exception": false,
     "start_time": "2020-09-26T11:53:15.297204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What next?\n",
    "### We overlooked the dataset exploration. Check for dataset imbalance and try to fix it using oversampling or undersampling \n",
    "### Divide the data into training and test data or perform cross validation\n",
    "\n",
    "### Test out different models and compare their validation accuracy and loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.258203,
     "end_time": "2020-09-26T11:53:16.074562",
     "exception": false,
     "start_time": "2020-09-26T11:53:15.816359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 59.241721,
   "end_time": "2020-09-26T11:53:16.445203",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-26T11:52:17.203482",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
